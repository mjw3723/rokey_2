# Doovis


## 📌 프로젝트 개요
본 프로젝트는 **Doosan M0609 로봇**과 ROS 2 Humble를 활용하여  
**공구**를 이용한 **음성인식 기반 공구 핸들링 로봇**을 구현하는 팀 프로젝트입니다.  
사용자의 음성 명령을 OpenAI의 음성 인식 모델을 통해 해석한 후, 해당 명령에 따라 YOLOv11 기반의 객체 인식 기술을 활용해 공구의 위치를 탐지하고 이를 정확하게 집어 전달하거나 제자리에 정리하는 과정을 자동으로 수행하는 지능형 헬퍼로봇을 구현함

---

## 🛠️ 사용 환경

| 항목         | 내용                          |
|--------------|-------------------------------|
| OS           | Ubuntu 22.04 LTS              |
| ROS2 Version | ROS2 Humble       |
| 사용 로봇     | Doosan M0609  |
| 음성인식    | open ai Wishper                    |
| TTS | open ai tts-1 |
| 객체 인식 | YOLO 11n |
| 관절 인식 | MediaPipe |
| 기타 요소     | 공구 세트  |

---

## 🧑‍🤝‍🧑 팀 구성

| 이름 | 역할 |
|------|------|
| 팀장 문준웅 | 주제 선정 및 프로젝트 기획 , 로봇 모션 제어 로직 구현  |
| 팀원 나승원 | 객체 인식 학습 모델 수집 , YOLOv11 기반 객체 인식 모델 튜닝 |
| 팀원 강인우 | 음성 인식 시스템 구현 , 객체검출 모델 개발 |
| 팀원 장연호 | OpenAI 프롬프트 설계 및 응답 튜닝 , YOLO 기반 신체 인식 모델 개발|

---

## 📅 작업 일정

| 날짜       | 작업 내용 요약                     |
|------------|------------------------------------|
| 2025.05.23 ~ 2025.05.25 | 프로젝트 기획 및 주제 선정 / 기획안 작성 → 사전 기획(아이디어 선정)     |
| 2025.05.26 ~ 2025.05.28 | 기능 설계 ,영상 데이터 수집 |
| 2025.05.29 ~ 2025.06.02  | 서브 테스크 작성 및 시험  |
| 2025.06.03 ~ 2025.06.04 | 전체 테스크 작성 및 시험   |
| 2025.06.04 ~ 2025.06.05| 테스트 및 발표자료 정리         |
| 개발 기간 | 총 13일        |

---

## 프로젝트 동작 흐름
![image](https://github.com/user-attachments/assets/80cc5724-db1e-435a-80fd-0fec87746351)

**설명**

왼쪽에 보이는 GPT 노드는 사용자의 음성 명령을 받아 키워드를 추출하는 역할을 합니다. 추출된 object와 target 정보를 Control 노드에 전달하면, Control은 시스템을 깨우기 위해 WakeUp_State 토픽을 발행하고 얼굴 위치 정보를 Detection 노드에 보냅니다. 이후 object에 대한 위치 정보를 서비스 형태로 요청하고, Detection 노드는 YOLO와 RealSense에서 받은 데이터를 바탕으로 객체를 인식해 위치 좌표를 응답합니다. 동시에 객체 박스 리스트를 토픽으로 발행하여 Control 노드가 후속 작업을 이어갈 수 있도록 합니다.

## wake up word

https://github.com/dscripka/openWakeWord
---
openWakeWord 프로젝트를 기반으로, 프로젝트 이름에 맞게 커스텀한 .tflite 모델을 제작해서 로봇의 음성 호출 인식에 사용. 호출 명령어는 hello doovis 

## 데이터 셋

base 데이터셋은 roboflow에 다른 사용자가 올린 데이터를 사용 , Annotation editor를 활용해서 직접 촬영한 데이터셋을 병합하는 방법을 사용

![image](https://github.com/user-attachments/assets/bd92dae3-9315-4192-b44b-aa85bdd5f1a7)

![image](https://github.com/user-attachments/assets/e49477f4-3887-4822-b2db-a1cb2e4bc84a)

"0": "hammer","1": "pliers","2": "screwdriver","3": "wrench", 4가지 클래스를 핸들링 할 수 있는 객체 검출 모델 제작

**모델 개선 전**

![image](https://github.com/user-attachments/assets/02b7baa9-07c3-460e-a8a5-508d2582153e)

**모델 개선 후**

![image](https://github.com/user-attachments/assets/e83eff53-f6fd-4818-a224-9c8551d0cc60)

---
## 관절인식 

![image](https://github.com/user-attachments/assets/07b154de-70f7-4d13-bf5d-333412ed95ff)

1. Face Tracking - 0번 Nose 관절번호를 사용해 Frame 픽셀을 로봇 좌표계로 변환하여 로봇이 사용자의 얼굴을 따라가도록 구현
2. Sholer - 12 번 어깨 관절번호를 사용해 공구를 가져올 때 사용자의 어깨 쪽으로 공구를 반환하도록 구현
3. Wrist - 16번 손목 관절 번호를 사용해 공구를 다시 제자리로 가져다 놓을때 사용자의 손목쪽으로 이동하여 공구를 받아가도록 구현

---

## 🚀 주요 기능

- **음성 명령 기반 Pick and Place** - 사용자가 말한 물체의 위치를 파악한 후 객체 검출 모델을 활용해 해당 물체를 인식하고 지정된 위치로 옮기는 기능
- **공구 가져오기, 가져가기** - 음성 인식을 통해 사용자의 신체 부위(오른쪽 어깨, 12번) 및 공구 종류를 파악한 후 객체 검출 모델을 활용해 원하는 공구를 사용자가 있는 곳으로 옮기는 기능
- **자연어 처리(일상대화 & 다중 인식)** - 여러 작업을 한번의 명령으로 진행할 수 있고  일상적인 대화가 가능할 수 있도록 구현
- **안전 기능(에러 핸들링 & 예외처리)** - 1. 운동 범위 제한 - 로봇이 이동하는 축 범위를 제한함으로써 사용자 following 중 특이점 확률을 낮춰 Pick and Place 시 안전한 이동이 가능하게 구현 <br>
2. 전체 상황 인식 - 사용자가 물건을 받지않거나 물건이 찾을 수 없을 때 제자리로 돌아가서 다음 동작을 수행

---

## 자체 피드백
**사진 기획 대비 달성도: 9점**
- 잘한점 : 물체에 위치에 따라 그리퍼가 회전하는 것을 구현하였고, 얼굴 트래킹을 좀 더 자연스럽고 빠르게 움직이게 하기위해 비동기로 모션을 구현하였다. 블렌딩을 통해 로봇의 움직임을 보다 효율적으로 개선
- 부족했던 점 : 사용자가 2명 인식되면 경고 메세지를 주는 기능이 시간이 지연되어 밀리거나 나오지않는 것을 해결하지 못한점이 아쉬운 부분이다.

---

## 시연영상

https://github.com/user-attachments/assets/61f985e3-9411-443b-bcc3-235c1610784b

https://github.com/user-attachments/assets/823db88c-1388-49bf-9dde-dfca57e5f58e

https://github.com/user-attachments/assets/38753a4d-eff7-40e3-9e45-65ecefad4ff5







